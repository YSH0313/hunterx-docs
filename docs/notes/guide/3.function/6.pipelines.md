---
title: pipeline功能
icon: fluent:pipeline-add-20-regular
createTime: 2025/01/14 19:15:10
permalink: /guide/ed1ln02o/
---

## 介绍

Pipeline（数据管道）用于对爬虫提取到的 Item 数据进行后续处理，如数据清洗、验证、去重和存储。它是爬虫数据流转的重要环节，确保数据符合需求并被正确保存。

## 功能扩展

- `数据清洗`：对爬取的数据进行格式化、去重和异常值处理，确保数据质量。
- `数据验证`：校验数据完整性和合法性，过滤无效或缺失信息的数据项。
- `灵活存储`：支持将数据存储到数据库（MySQL、MongoDB）、文件（CSV、JSON）或分布式存储系统。
- `多层处理`：支持多级管道，按优先级顺序依次处理数据，灵活组合不同功能。
- `易于扩展`：可自定义管道，按需实现特定的数据处理逻辑。

:::warning
框架内部并不提供 `数据清洗` `数据验证` 功能，如果有需要可以根据需求自行添加相关规则
:::

## 处理流程

- `Item 提交`：爬虫提取数据并提交给管道。
- `数据清洗`：管道对数据进行清洗、格式化和去重。
- `数据验证`：校验数据合法性，不符合规则的数据将被丢弃。
- `数据存储`：将有效数据写入数据库或文件。

## 内置函数

<table>
  <thead>
    <tr>
      <th>类名</th>
      <th>方法名</th>
      <th>传参</th>
      <th>说明</th>
      <th>注意</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="2">Pipeline</td>
      <td rowspan="2">process_item</td>
      <td>item</td>
      <td>接受传递的 item 值，Object 格式</td>
      <td rowspan="2">所有自定义pipeline都需要继承Pipeline类并继承process_item方法</td>
    </tr>
    <tr>
      <td>spider</td>
      <td>接受的爬虫对象，便于精细化记录</td>
    </tr>
  </tbody>
</table>

## 使用示例

:::tabs
@tab :[devicon:python]: pipelines.py

```python
from hunterx.piplines.basepipeline import Pipeline
from items import *


class FirstSpiderPipeline(Pipeline):

    async def process_item(self, item, spider):
        if isinstance(item, FirstSpiderItem):
            print(item)
            print(spider.name)
            # 更多处理逻辑
```

@tab :[devicon:python]: items.py

```python
from hunterx.items.baseitem import Item, dataclass, field


@dataclass
class FirstSpiderItem(Item):
    title: str = field(default="")
    number: int = field(default=0)
```

@tab :[devicon:python]: first_spider.py

```python
# -*- coding: utf-8 -*-
import hunterx
from hunterx.spiders import MemorySpider
from items import FirstSpiderItem


class FirstSpiderSpider(MemorySpider):
    name = 'first_spider'

    def __init__(self):
        self.header = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'
        }

    def start_requests(self):
        url = 'https://www.example.com/'
        yield hunterx.Requests(url=url, headers=self.header, callback=self.parse, level=1)

    async def parse(self, response):
        item = FirstSpiderItem()
        item.title = 'title_example'
        item.number = 10
        yield item


if __name__ == '__main__':
    start_run = FirstSpiderSpider()
    start_run.run()
```

@tab :[devicon:python]: settings.py

```python
# 数据处理管道，可创建多个，数字越小优先级越高
ITEM_PIPELINES = {
    'FirstSpiderPipeline': 100
}
```

:::

:::tip
记得在 `settings.py` 中添加上定义好的pipeline类哦
:::

## 总结

Pipeline 是数据处理的关键环节，具备强大的数据清洗、验证和存储功能，灵活支持多种存储方式。通过优先级和多层管道机制，轻松实现复杂的数据处理流程。
